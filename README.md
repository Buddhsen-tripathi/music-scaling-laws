# Scaling Laws for Music Language Models

A study of transformer vs. LSTM scaling on symbolic music (ABC notation). Includes data preprocessing, model training, scaling-law plots, and music generation experiments. 

## Project Goals
- The project builds a full preprocessing pipeline for ABC music
- Trains transformers of varying sizes (1Mâ€“100M params)
- Trains RNNs/LSTMs of similar sizes
- Derives scaling laws (loss vs model size)
- Generates symbolic music samples

## Repository Structure
- README.md

## Running the Code
Detailed instructions will be added as the project scripts are completed.